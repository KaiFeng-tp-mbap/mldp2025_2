## Import libraries
import matplotlib.pyplot as plt
import numpy as np

# import pandas library
import pandas as pd
import seaborn as sns

## Read *.csv file into pandas DataFrame
FILE_PATH = "ifood_df.csv"
df = pd.read_csv(FILE_PATH)

df ## Display dataframe
## Predicting Classification of marketing data of target variable 'Response' if customer will respond to a last marketing campaign or not

## Understand the type of variable for each column
df.dtypes

## Check for missing data
df.isna().sum()

## Describe data distribution
df.describe(include = "all")

## Understanding distribution of target
col_y = 'Response' # Choose target variable

## Understanding distribution of features
df.hist(figsize=(20,20))
plt.show()

# Tally unique values
df[col_y].value_counts()
plt.show()

# plot bar chart
df[col_y].value_counts().plot(kind='bar')
plt.title(f'Distribution of {col_y}')
plt.legend()
plt.xlabel("")
plt.ylabel("Number of response")
plt.show()# class imbalance

## Plot boxplot for different features grouped by Response
for col in df.columns:
    if df[col].dtype==int or df[col].dtype==float:
 
        df.boxplot(column=col,
                   by='Response',
                   grid=False,
                   rot=90,
                   figsize=(4,2))
        plt.title(f"Boxplot of {col}")
        plt.suptitle('')  # Remove default title by Pandas
        plt.xlabel("Response")
        plt.ylabel("Number of samples")
        plt.show()

## Understanding relationship between variables
df.corr()

sns.pairplot(df, hue='Response') ## Pairplot of all columns colored by Response
plt.show()

# Response 0 has more votes than Response 1 across all features
# There are outliers in the dataset

## Clean data
col_irrelevant = [col_y,'AcceptedCmp1','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5','AcceptedCmpOverall','Customer_Days']  # List of irrelevant columns
X = df.drop(col_irrelevant, axis=1)  #remove irrelevant columns (including target)
X

## Handling Categorical Data (One-Hot Encoding)
y = df[col_y]  ## Select target column 

## One-Hot Encoding
X = pd.get_dummies(X, drop_first=True)  # encode categorical columns
X

## Plot boxplot for different features grouped by Response
for col in X.columns:
    if col != col_y and (X[col].dtype==int or X[col].dtype==float):
        #Calling mean to cap outliers
        Q1 = X[col].quantile(0.25)
        Q3 = X[col].quantile(0.75)
        IQR = Q3 - Q1
        upper_bound = Q3 + 1.5 * IQR

        X.loc[ X[col] > upper_bound, col] = upper_bound.astype(X[col].dtype)  # cap outlier in Income column



df[col_y].value_counts()
## Plot boxplot for different features grouped by Response
for col in X.columns:
    if col != col_y and (X[col].dtype==int or X[col].dtype==float):

        df.boxplot(column=col,
                   by='Response',
                   grid=False,
                   rot=90,
                   figsize=(4,2))
        plt.title(f"Boxplot of {col}")
        plt.suptitle('')  # Remove default title by Pandas
        plt.xlabel("Response")
        plt.ylabel("Number of samples")
        plt.show()

## Split data into train set and test set
from sklearn.model_selection import train_test_split
test_size = 0.5
random_state = 20 ## for reproducibility
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state) # Split the data

## Initialise and train model
from sklearn.tree import DecisionTreeClassifier

## Initialise model
dt = DecisionTreeClassifier()  # Initialise Decision Tree Classifier

## Train model
dt.fit(X_train, y_train)  # pass training set to model

from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
## Initialise the models
logr = LogisticRegression(max_iter=100)  # Initialise Logistic Regression
lda = LinearDiscriminantAnalysis()
## Train model
logr.fit(X_train, y_train)  # pass training set to model

# Randomized Search CV
from scipy.stats import randint, uniform

# Randomized Search CV parameters
param_dist_rf = {
    'n_estimators': randint(1, 5), # create randint of range 1 to 5 trees 
    'criterion': ['gini', 'entropy'], #measure quality of split, supported criteria: 'gini' for gini impurity and 'entropy
    'max_depth': randint(1, 3), # maximum depth of tree
    'min_samples_split': randint(2, 5), # minimum number of samples requied to split internal node
    'max_features': ['sqrt', 'log2'], #number of features to consider when looking for best split
    'max_samples': uniform(0.6, 0.4)
}

model = dt  # TODO: choose model for prediction

## Save model
import joblib
joblib.dump(dt, 'ifood_df_model.joblib')  # TODO: Save model using joblib.dump(...)

# Gradient Boosted Tree
from sklearn.ensemble import GradientBoostingClassifier
gbt = GradientBoostingClassifier()  # Initialise GradientBoostingClassifier
gbt.fit(X_train, y_train)  #  gbt.fit(...)

# Random Forest
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()  # TODO: Initialise RandomForestClassifier
rf.fit(X_train, y_train)  # TODO: rf.fit(...)

pd.Series(y_pred_rf).value_counts().plot(kind='bar')

plt.title(f'Distribution of Response after Random Forest Prediction')
plt.legend()
plt.xlabel("")
plt.ylabel("Number of response")
plt.show()

pd.Series(y_pred_gbt).value_counts().plot(kind='bar')

plt.title(f'Distribution of Response after Gradient Boosting Prediction')
plt.legend()
plt.xlabel("")
plt.ylabel("Number of response")
plt.show()


# 5. Model Evaluation

## Make predictions on the test set using X_test
from sklearn.metrics import accuracy_score
y_pred_dt = dt.predict(X_test)  #  dt.predict(...) pre
print("Decision Tree Accuracy: ", accuracy_score(y_test, y_pred_dt))  # use accuracy_score(...)

#calculate training loss to measure how far predictions are from actual values
metric =  accuracy_score
trained_metricdt = metric(y_train, dt.predict(X_train))
#train metric    
print("Decision Tree Training loss: ",trained_metricdt)

test_metricdt = metric(y_test, dt.predict(X_test))
#test metric
print("Decision Tree Test loss: ",test_metricdt)
# This indicates overfitting if training loss is significantly lower than test loss

## Make predictions on the test set using X_test

## Evaluate model
## Make predictions on the test set using X_test
y_pred = logr.predict(X_test)

## Calculate the accuracy of the model
from sklearn.metrics import accuracy_score
print("Logistic Regression: ", accuracy_score(y_test, y_pred))

#calculate training loss to measure how far predictions are from actual values
metric =  accuracy_score

trained_metriclogr = metric(y_train, logr.predict(X_train))
#train metric    
print("Logistic Regression Training loss: ",trained_metriclogr)

test_metriclogr = metric(y_test, logr.predict(X_test))
#test metric
print("Logistic Regression Test loss: ",test_metriclogr)
# indicates good fit

# Random Forest Predictions
y_pred_rf = rf.predict(X_test)  # TODO: rf.predict(...)
print("Random Forest Accuracy: ", accuracy_score(y_test, y_pred_rf))
joblib.dump(rf, 'ifood_df_rf_model.joblib')  # Save model

#calculate training loss to measure how far predictions are from actual values
metric =  accuracy_score

trained_metricrf = metric(y_train, rf.predict(X_train))
#train metric    
print("Random Forest Training loss: ",trained_metricrf)

test_metricrf = metric(y_test, rf.predict(X_test))
#test metric
print("Random Forest Test loss: ",test_metricrf)
# This idicates overfitting if training loss is significantly lower than test loss

# Gradient Boosted Tree Predictions
y_pred_gbt = gbt.predict(X_test)  #  gbt.predict(...)
print("Gradient Boosted Tree Accuracy: ", accuracy_score(y_test, y_pred_gbt))
joblib.dump(gbt, 'ifood_df_gbt_model.joblib')  #  Save model

#calculate training loss to measure how far predictions are from actual values
metric =  accuracy_score

trained_metricgbt = metric(y_train, gbt.predict(X_train))
#train metric    
print("Gradient Boosting Training loss: ",trained_metricgbt)

test_metricgbt = metric(y_test, gbt.predict(X_test))
#test metric
print("Gradient Boosting Test loss: ",test_metricgbt)

# RandomizedSearchCV
from sklearn.model_selection import RandomizedSearchCV
rs_rf = RandomizedSearchCV(
    estimator = rf, # initialized model
    param_distributions = param_dist_rf,#Search grid (list of parameters)
    cv = 5, # 5 folds for cross validation
    scoring = 'neg_mean_squared_error', # Evaluation used to Evaluate validation set
    n_iter = 10, # number of different combinations to try out
    n_jobs = -1 # Controlling how many CPU cores to use -1: Use all available CPU cores
)
rs_rf.fit(X_train, y_train) # pass training set to model

best_rs_params = rs_rf.best_params_  # TODO: rs_rf.best_params_
best_rs_rf = rs_rf.best_estimator_      # TODO: rs_rf.best_estimator_
print("Best Parameters: ", best_rs_params)
print("Best Estimator: ", best_rs_rf)

## choosing the best model to predict is random forest as it has the highest accuracy
## New data for Random Forest prediction
X_unseen_rf = pd.read_csv(FILE_PATH)  # Again CSV
col_df_X = df.drop(col_y, axis=1).columns  # choose features
col_ohe = X.columns.tolist()
X_unseen_rf = pd.DataFrame(X_unseen_rf, columns=col_df_X)
X_unseen_rf = pd.get_dummies(X_unseen_rf)  #  OHE
X_unseen_rf = X_unseen_rf.reindex(columns=col_ohe, fill_value=0)
display(X_unseen_rf)

## Predict for Random Forest
X_unseen_rf['Prediction'] = rf.predict(X_unseen_rf)  # TODO: dt.predict(...)
y_unseen_rf = pd.read_csv(FILE_PATH)
X_unseen_rf['Actual'] = y_unseen_rf[col_y]  # TODO: y_unseen[col_y]
X_unseen_rf

# Using Barplot to show the distribution of predicted responses after Random Forest Prediction
# tally predictions
pd.Series(X_unseen_rf['Prediction']).value_counts().plot(kind='bar')
plt.title(f'Distribution of Predicted Response after Random Forest Prediction on New Data')
plt.legend()
plt.xlabel("")
plt.ylabel("Number of response")
plt.show()

# Using Barplot to show the distribution of actual responses after Random Forest Prediction
# tally predictions
pd.Series(X_unseen_rf['Actual']).value_counts().plot(kind='bar')
plt.title(f'Distribution of Actual Response after Random Forest Actual Prediction on New Data')
plt.legend()
plt.xlabel("")
plt.ylabel("Number of response")
plt.show()

#predict on training ste
y_train_pred = rf.predict(X_train)  # TODO: model.predict(...)

# make a copy of X_train to store predictions
X_train_pred = X_train.copy()
X_train_pred['Prediction'] = y_train_pred  # TODO: assign y_train_pred to new column 'Prediction'
X_train_pred
X_train_pred['Actual'] = y_train  # TODO: assign y_train to new column 'Actual'
X_train_pred

#predict on test set
y_test_pred = rf.predict(X_test)  # TODO: model.predict(...)
# make a copy of X_test to store predictions
X_test_pred = X_test.copy()
X_test_pred['Prediction'] = y_test_pred  # TODO: assign y_test_pred to new column 'Prediction'
X_test_pred
X_test_pred['Actual'] = y_test  # TODO: assign y_test to new column 'Actual'
X_test_pred

# Using Barplot to show the distribution of predicted responses after Random Forest Prediction
# tally predictions
pd.Series(X_train_pred['Prediction']).value_counts().plot(kind='bar')
plt.title(f'Distribution of Predicted Response after Random Forest Prediction on training Data')
plt.legend()
plt.xlabel("")
plt.ylabel("Number of response")
plt.show()

# Using Barplot to show the distribution of predicted responses after Random Forest Prediction
# tally predictions
pd.Series(X_train_pred['Actual']).value_counts().plot(kind='bar')
plt.title(f'Distribution of Predicted Response after Random Forest Prediction on training Data')
plt.legend()
plt.xlabel("")
plt.ylabel("Number of response")
plt.show()

# Using Barplot to show the distribution of actual responses after Random Forest Prediction
# tally predictions
pd.Series(X_test_pred['Prediction']).value_counts().plot(kind='bar')
plt.title(f'Distribution of Predicted Response after Random Forest Prediction on test Data')
plt.legend()
plt.xlabel("")
plt.ylabel("Number of response")
plt.show()

# Using Barplot to show the distribution of actual responses after Random Forest Prediction
# tally predictions
pd.Series(X_test_pred['Actual']).value_counts().plot(kind='bar')
plt.title(f'Distribution of Actual Response after Random Forest Actual Prediction on test Data')
plt.legend()
plt.xlabel("")
plt.ylabel("Number of response")
plt.show()

# Confusion Matrix for Random Forest
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_pred_rf) 
# True Negative: 917
# False Positive: 32
# False Negative: 101
# True Positive: 53 

# classification_report for Random Forest
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_rf))

## Further feature engineering / feature selection
# Principal Component Analysis
from sklearn.decomposition import PCA
pca = PCA(n_components=3)
pca.fit(X)

#one hot encoding categorical columns
X_pca = pca.transform(X)
X_pca 

# create a DataFrame for PCA transformed data
df_pca = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2', 'PC3'])
print("PCA transformed DataFrame:")
df_pca

# Clustering using KMeans
# Using Clustering for feature engineering to add cluster labels as new features
# using enriched dataset (original dataset + cluster labels) in supervised learning for classification
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3, random_state=0)
kmeans.fit(X)

# Get cluster labels for each data point
cluster_labels = kmeans.labels_

## Elbow method to find optimal number of clusters
WCSS = []
 
for clusters in range(1,10):
    kmeans = KMeans(n_clusters=clusters)
    kmeans.fit(X)
    WCSS.append(kmeans.inertia_)
 
plt.plot(range(1,10), WCSS)
plt.title('The elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()
#choose 3 clusters based on elbow method
#WCSS decreases significantly up to 3 clusters, after which the decrease becomes more gradual
