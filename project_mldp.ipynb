## Import libraries
import matplotlib.pyplot as plt
import numpy as np

# import pandas library
import pandas as pd
import seaborn as sns

## Read *.csv file into pandas DataFrame
FILE_PATH = "ifood_df.csv"
df = pd.read_csv(FILE_PATH)
df = df.drop(['AcceptedCmp1','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5','AcceptedCmpOverall','Recency','Customer_Days'], axis=1)
df ## Display dataframe
## Predicting Classification of marketing data of target variable 'Response' if customer will respond to a last marketing campaign or not

## Understand the type of variable for each column
df.dtypes

## Check for missing data
df.isna().sum()

## Describe data distribution
df.describe(include = "all")

## Understanding distribution of target
col_y = 'Response' # Choose target variable

## Understanding distribution of features
df.hist(figsize=(20,20))
plt.show()

# Tally unique values
df[col_y].value_counts()
plt.show()

# plot bar chart
df[col_y].value_counts().plot(kind='bar')
plt.title(f'Distribution of {col_y}')
plt.legend()
plt.xlabel("")
plt.ylabel("Number of response")
plt.show()# class imbalance

## Understanding relationship between variables
df.corr()

## Clean data
X = df.drop(col_y, axis=1)  #remove irrelevant columns (including target)

## Handling Categorical Data (One-Hot Encoding)
y = df[col_y]  ## Select target column 

## One-Hot Encoding
X = pd.get_dummies(X, drop_first=True)  # encode categorical columns
X

## Split data into train set and test set
from sklearn.model_selection import train_test_split
test_size = 0.5
random_state = 20 ## for reproducibility
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state) # Split the data

## Initialise and train model
from sklearn.tree import DecisionTreeClassifier

## Initialise model
dt = DecisionTreeClassifier()  # Initialise Decision Tree Classifier

## Train model
dt.fit(X_train, y_train)  

from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
## Initialise the models
logr = LogisticRegression(max_iter=100)  # Initialise Logistic Regression
lda = LinearDiscriminantAnalysis()
## Train model
logr.fit(X_train, y_train)  


from sklearn.neural_network import MLPClassifier ## For classification

mlp = MLPClassifier(
    hidden_layer_sizes=(2,32),
    activation='relu',
    solver='adam',
    learning_rate_init = 0.001,
    max_iter=200,
)
mlp.fit(X_train, y_train)


## Evaluate model
## Make predictions on the test set using X_test
from sklearn.metrics import accuracy_score
y_pred_dt = dt.predict(X_test)  #  dt.predict(...) pre
print("Decision Tree Accuracy: ", accuracy_score(y_test, y_pred_dt))  # use accuracy_score(...)

## Make predictions on the test set using X_test

## Evaluate model
## Make predictions on the test set using X_test
y_pred = logr.predict(X_test)

## Calculate the accuracy of the model
from sklearn.metrics import accuracy_score
print("Logistic Regression: ", accuracy_score(y_test, y_pred))

# Randomized Search CV
from scipy.stats import randint, uniform

param_dist_rf = {
    'n_estimators': randint(1, 5),
    'criterion': ['gini', 'entropy'],
    'max_depth': randint(1, 3),
    'min_samples_split': randint(2, 5),
    'max_features': ['sqrt', 'log2'],
    'max_samples': uniform(0.6, 0.4)
}

model = dt  # TODO: choose model for prediction

## Save model
import joblib
joblib.dump(dt, 'ifood_df_model.joblib')  # TODO: Save model using joblib.dump(...)

# Gradient Boosted Tree
from sklearn.ensemble import GradientBoostingClassifier
gbt = GradientBoostingClassifier()  # Initialise GradientBoostingClassifier
gbt.fit(X_train, y_train)  #  gbt.fit(...)
y_pred_gbt = gbt.predict(X_test)  #  gbt.predict(...)
print("Gradient Boosted Tree Accuracy: ", accuracy_score(y_test, y_pred_gbt))
joblib.dump(gbt, 'ifood_df_gbt_model.joblib')  #  Save model

# Random Forest
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()  # TODO: Initialise RandomForestClassifier
rf.fit(X_train, y_train)  # TODO: rf.fit(...)
y_pred_rf = rf.predict(X_test)  # TODO: rf.predict(...)
print("Random Forest Accuracy: ", accuracy_score(y_test, y_pred_rf))
joblib.dump(rf, 'ifood_df_rf_model.joblib')  # TODO: Save model

from sklearn.model_selection import RandomizedSearchCV
rs_rf = RandomizedSearchCV(
    estimator = rf,
    param_distributions = param_dist_rf,
    cv = 5,
    scoring = 'neg_mean_squared_error',
    n_iter = 10,
    n_jobs = -1
)
rs_rf.fit(X_train, y_train)

# Confusion Matrix
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_pred_dt) 
# True Negative: 811
# False Positive: 138
# False Negative: 98
# True Positive: 56

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_dt))


## New data
X_unseen = pd.read_csv(FILE_PATH)  # Again CSV
col_df_X = df.drop(col_y, axis=1).columns  # choose features
col_ohe = X.columns.tolist()
X_unseen = pd.DataFrame(X_unseen, columns=col_df_X)
X_unseen = pd.get_dummies(X_unseen)  #  OHE
X_unseen = X_unseen.reindex(columns=col_ohe, fill_value=0)
display(X_unseen)


## Predict
X_unseen['Prediction'] = dt.predict(X_unseen)  # TODO: dt.predict(...)
y_unseen = pd.read_csv(FILE_PATH)
X_unseen['Actual'] = y_unseen[col_y]  # TODO: y_unseen[col_y]
X_unseen

## Further feature engineering / feature selection
# Principal Component Analysis
from sklearn.decomposition import PCA
pca = PCA(n_components=3)
pca.fit(X)
