## Import libraries
import matplotlib.pyplot as plt
import numpy as np

# import pandas library
import pandas as pd
import seaborn as sns

## Read *.csv file into pandas DataFrame
FILE_PATH = "ifood_df.csv"
df = pd.read_csv(FILE_PATH)

df ## Display dataframe
## Predicting Classification of marketing data of target variable 'Response' if customer will respond to a last marketing campaign or not

## Understand the type of variable for each column
df.dtypes

## Check for missing data
df.isna().sum()

## Describe data distribution
df.describe(include = "all")

## Understanding distribution of target
col_y = 'Response' # Choose target variable

## Understanding distribution of features
df.hist(figsize=(20,20))
plt.show()

# Tally unique values
df[col_y].value_counts()
plt.show()

# plot bar chart
df[col_y].value_counts().plot(kind='bar')
plt.title(f'Distribution of {col_y}')
plt.legend()
plt.xlabel("")
plt.ylabel("Number of response")
plt.show()# class imbalance

## Plot boxplot for different features grouped by Response
for col in df.columns:
    if df[col].dtype==int or df[col].dtype==float:
 
        df.boxplot(column=col,
                   by='Response',
                   grid=False,
                   rot=90,
                   figsize=(4,2))
        plt.title(f"Boxplot of {col}")
        plt.suptitle('')  # Remove default title by Pandas
        plt.xlabel("Response")
        plt.ylabel("Number of samples")
        plt.show()

## Understanding relationship between variables
df.corr()

sns.pairplot(df, hue='Response') ## Pairplot of all columns colored by Response
plt.show()

# Response 0 has more votes than Response 1 across all features
# There are outliers in the dataset

## Clean data
col_irrelevant = [col_y,'AcceptedCmp1','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5','AcceptedCmpOverall','Customer_Days']  # List of irrelevant columns
X = df.drop(col_irrelevant, axis=1)  #remove irrelevant columns (including target)
X

## Handling Categorical Data (One-Hot Encoding)
y = df[col_y]  ## Select target column 

## One-Hot Encoding
X = pd.get_dummies(X, drop_first=True)  # encode categorical columns
X

## Plot boxplot for different features grouped by Response
for col in df.columns:
    if col != col_y and (df[col].dtype==int or df[col].dtype==float):
        #Calling mean to cap outliers
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        upper_bound = Q3 + 1.5 * IQR

        df.loc[ df[col] > upper_bound, col] = upper_bound.astype(df[col].dtype)  # cap outlier in Income column


df[col_y].value_counts()
## Plot boxplot for different features grouped by Response
for col in df.columns:
    if col != col_y and (df[col].dtype==int or df[col].dtype==float):

        df.boxplot(column=col,
                   by='Response',
                   grid=False,
                   rot=90,
                   figsize=(4,2))
        plt.title(f"Boxplot of {col}")
        plt.suptitle('')  # Remove default title by Pandas
        plt.xlabel("Response")
        plt.ylabel("Number of samples")
        plt.show()

## Split data into train set and test set
from sklearn.model_selection import train_test_split
test_size = 0.5
random_state = 20 ## for reproducibility
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state) # Split the data

## Initialise and train model
from sklearn.tree import DecisionTreeClassifier

## Initialise model
dt = DecisionTreeClassifier()  # Initialise Decision Tree Classifier

## Train model
dt.fit(X_train, y_train)  # pass training set to model

from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
## Initialise the models
logr = LogisticRegression(max_iter=100)  # Initialise Logistic Regression
lda = LinearDiscriminantAnalysis()
## Train model
logr.fit(X_train, y_train)  # pass training set to model


from sklearn.neural_network import MLPClassifier ## For classification

mlp = MLPClassifier(
    hidden_layer_sizes=(2,32),
    activation='relu',
    solver='adam',
    learning_rate_init = 0.001,
    max_iter=200,
)
mlp.fit(X_train, y_train) # pass training set to model


## Evaluate model
## Make predictions on the test set using X_test
from sklearn.metrics import accuracy_score
y_pred_dt = dt.predict(X_test)  #  dt.predict(...) pre
print("Decision Tree Accuracy: ", accuracy_score(y_test, y_pred_dt))  # use accuracy_score(...)

## Make predictions on the test set using X_test

## Evaluate model
## Make predictions on the test set using X_test
y_pred = logr.predict(X_test)

## Calculate the accuracy of the model
from sklearn.metrics import accuracy_score
print("Logistic Regression: ", accuracy_score(y_test, y_pred))

# Randomized Search CV
from scipy.stats import randint, uniform

# Randomized Search CV parameters
param_dist_rf = {
    'n_estimators': randint(1, 5), # create randint of range 1 to 5 trees 
    'criterion': ['gini', 'entropy'], #measure quality of split, supported criteria: 'gini' for gini impurity and 'entropy
    'max_depth': randint(1, 3), # maximum depth of tree
    'min_samples_split': randint(2, 5), # minimum number of samples requied to split internal node
    'max_features': ['sqrt', 'log2'], #number of features to consider when looking for best split
    'max_samples': uniform(0.6, 0.4)
}

model = dt  # TODO: choose model for prediction

## Save model
import joblib
joblib.dump(dt, 'ifood_df_model.joblib')  # TODO: Save model using joblib.dump(...)

# Gradient Boosted Tree
from sklearn.ensemble import GradientBoostingClassifier
gbt = GradientBoostingClassifier()  # Initialise GradientBoostingClassifier
gbt.fit(X_train, y_train)  #  gbt.fit(...)
y_pred_gbt = gbt.predict(X_test)  #  gbt.predict(...)
print("Gradient Boosted Tree Accuracy: ", accuracy_score(y_test, y_pred_gbt))
joblib.dump(gbt, 'ifood_df_gbt_model.joblib')  #  Save model

# Random Forest
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()  # TODO: Initialise RandomForestClassifier
rf.fit(X_train, y_train)  # TODO: rf.fit(...)
y_pred_rf = rf.predict(X_test)  # TODO: rf.predict(...)
print("Random Forest Accuracy: ", accuracy_score(y_test, y_pred_rf))
joblib.dump(rf, 'ifood_df_rf_model.joblib')  # Save model

pd.Series(y_pred_rf).value_counts().plot(kind='bar')

plt.title(f'Distribution of Response after Random Forest Prediction')
plt.legend()
plt.xlabel("")
plt.ylabel("Number of response")
plt.show()

pd.Series(y_pred_gbt).value_counts().plot(kind='bar')

plt.title(f'Distribution of Response after Gradient Boosting Prediction')
plt.legend()
plt.xlabel("")
plt.ylabel("Number of response")
plt.show()

# RandomizedSearchCV
from sklearn.model_selection import RandomizedSearchCV
rs_rf = RandomizedSearchCV(
    estimator = rf, # initialized model
    param_distributions = param_dist_rf,#Search grid (list of parameters)
    cv = 5, # 5 folds for cross validation
    scoring = 'neg_mean_squared_error', # Evaluation used to Evaluate validation set
    n_iter = 10, # number of different combinations to try out
    n_jobs = -1 # Controlling how many CPU cores to use -1: Use all available CPU cores
)
rs_rf.fit(X_train, y_train) # pass training set to model

# Confusion Matrix
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_pred_dt) 
# True Negative: 811
# False Positive: 138
# False Negative: 98
# True Positive: 56

# classification_report
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_dt))


## New data for decision Tree prediction
X_unseen = pd.read_csv(FILE_PATH)  # Again CSV
col_df_X = df.drop(col_y, axis=1).columns  # choose features
col_ohe = X.columns.tolist()
X_unseen = pd.DataFrame(X_unseen, columns=col_df_X)
X_unseen = pd.get_dummies(X_unseen)  #  OHE
X_unseen = X_unseen.reindex(columns=col_ohe, fill_value=0)
display(X_unseen)


## Predict for decision Tree
X_unseen['Prediction'] = dt.predict(X_unseen)  # TODO: dt.predict(...)
y_unseen = pd.read_csv(FILE_PATH)
X_unseen['Actual'] = y_unseen[col_y]  # TODO: y_unseen[col_y]
X_unseen

# Tally unique values for Decision Tree
X_unseen['Prediction'].value_counts()
plt.show()

# plot bar chart
X_unseen['Prediction'].value_counts().plot(kind='bar')
plt.title(f'Distribution of Prediction (Decision Tree)')
plt.legend()
plt.xlabel("")
plt.ylabel("Number of response")
plt.show()# class imbalance

# Tally unique values for Decision Tree
X_unseen['Actual'].value_counts()
plt.show()

# plot bar chart
X_unseen['Actual'].value_counts().plot(kind='bar')
plt.title(f'Distribution of Actual (Decision Tree)')
plt.legend()
plt.xlabel("")
plt.ylabel("Number of response")
plt.show()# class imbalance

# Confusion Matrix for Decision Tree
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_pred_dt) 
# True Negative: 823
# False Positive: 126
# False Negative: 91
# True Positive: 63 y_pred

# classification_report for Decision Tree
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_dt))

## New data for Logistic Regression prediction
X_unseen_logr = pd.read_csv(FILE_PATH)  # Again CSV
col_df_X = df.drop(col_y, axis=1).columns  # choose features
col_ohe = X.columns.tolist()
X_unseen_logr = pd.DataFrame(X_unseen_logr, columns=col_df_X)
X_unseen_logr = pd.get_dummies(X_unseen_logr)  #  OHE
X_unseen_logr = X_unseen_logr.reindex(columns=col_ohe, fill_value=0)
display(X_unseen_logr)

## Predict for Logistic Regression
X_unseen_logr['Prediction'] = logr.predict(X_unseen_logr)  # TODO: dt.predict(...)
y_unseen_logr = pd.read_csv(FILE_PATH)
X_unseen_logr['Actual'] = y_unseen_logr[col_y]  # TODO: y_unseen[col_y]
X_unseen_logr

# Tally unique values for Logistic Regression
X_unseen_logr['Prediction'].value_counts()
plt.show()

# plot bar chart
X_unseen_logr['Prediction'].value_counts().plot(kind='bar')
plt.title(f'Distribution of Prediction (Logistic Regression)')
plt.legend()
plt.xlabel("")
plt.ylabel("Number of response")
plt.show()# class imbalance


# Tally unique values for Logistic Regression
X_unseen_logr['Actual'].value_counts()
plt.show()

# plot bar chart
X_unseen_logr['Actual'].value_counts().plot(kind='bar')
plt.title(f'Distribution of Actual (Logistic Regression)')
plt.legend()
plt.xlabel("")
plt.ylabel("Number of response")
plt.show()# class imbalance

# Confusion Matrix for logistic regression
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_pred) 
# True Negative: 919
# False Positive: 30
# False Negative: 121
# True Positive: 33 


# Confusion Matrix for logistic regression
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_pred) 
# True Negative: 919
# False Positive: 30
# False Negative: 121
# True Positive: 33 

## Further feature engineering / feature selection
# Principal Component Analysis
from sklearn.decomposition import PCA
pca = PCA(n_components=3)
pca.fit(X)
